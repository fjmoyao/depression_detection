{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Proyectos\\TESIS 2024\\depression_detection\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path \n",
    "import os \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se leen los datos y se seleccionan las variables que presentan una diferencia\n",
    "# entre los individuos que presentan estres y los que no\n",
    "data_path = Path(os.getcwd()).parent / \"data\"\n",
    "silver_path = data_path / \"silver\" / \"dreadditCleanTrain.csv\"\n",
    "df = pd.read_csv(silver_path, usecols= [\"text\",\"clean_text\",\"clean_text_sentence_sep\",\n",
    "                                        \"singular_pronouns\", \"avg_word_len\",'lex_diversity',\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de cómo usar el dataset\n",
    "dataset = SentimentDataset(df['clean_text'].tolist(), df['label'].tolist(), tokenizer, max_length=128)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que 'model' y 'loader' están definidos y configurados correctamente\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Listas para almacenar las métricas\n",
    "losses = []\n",
    "f1_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, device, num_epochs=5):\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    best_f1 = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Evaluación\n",
    "        model.eval()\n",
    "        val_true, val_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                logits = outputs.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                val_true.extend(labels.cpu().numpy())\n",
    "                val_preds.extend(predictions.cpu().numpy())\n",
    "\n",
    "        val_f1 = f1_score(val_true, val_preds, average='binary')\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1  # Actualizar el mejor F1 score visto hasta ahora\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation F1: {val_f1:.4f}')\n",
    "\n",
    "    return best_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Epoch 1/3, Train Loss: 0.6318, Validation F1: 0.8197\n",
      "Epoch 2/3, Train Loss: 0.3882, Validation F1: 0.8008\n",
      "Epoch 3/3, Train Loss: 0.3020, Validation F1: 0.8430\n",
      "Training fold 2\n",
      "Epoch 1/3, Train Loss: 0.3104, Validation F1: 0.9565\n",
      "Epoch 2/3, Train Loss: 0.2282, Validation F1: 0.9270\n",
      "Epoch 3/3, Train Loss: 0.1274, Validation F1: 0.9327\n",
      "Training fold 3\n",
      "Epoch 1/3, Train Loss: 0.1697, Validation F1: 0.9946\n",
      "Epoch 2/3, Train Loss: 0.0892, Validation F1: 0.9788\n",
      "Epoch 3/3, Train Loss: 0.0570, Validation F1: 0.9661\n",
      "Average F1 across folds: 0.9314\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "# Inicializar el KFold\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Convertir labels a numpy para el uso con StratifiedKFold\n",
    "labels = np.array(df['label'].tolist())\n",
    "\n",
    "# Ejecutar la validación cruzada\n",
    "f1_scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
    "    train_subsampler = Subset(dataset, train_idx)\n",
    "    val_subsampler = Subset(dataset, val_idx)\n",
    "    \n",
    "    train_loader = DataLoader(train_subsampler, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_subsampler, batch_size=16, shuffle=False)\n",
    "\n",
    "    print(f\"Training fold {fold+1}\")\n",
    "    fold_f1 = train_and_evaluate(model, train_loader, val_loader, device)\n",
    "    f1_scores.append(fold_f1)\n",
    "\n",
    "print(f\"Average F1 across folds: {np.mean(f1_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_tokenizer(model, tokenizer, save_directory):\n",
    "    \"\"\"\n",
    "    Save the fine-tuned model and tokenizer to a specified directory.\n",
    "\n",
    "    Args:\n",
    "        model: The trained RoBERTa model (e.g., an instance of RobertaForSequenceClassification).\n",
    "        tokenizer: The tokenizer used with the RoBERTa model.\n",
    "        save_directory (str): The path to the directory where the model and tokenizer will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Create the directory if it does not exist\n",
    "    import os\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    # Save the model\n",
    "    model.save_pretrained(save_directory)\n",
    "    \n",
    "    # Save the tokenizer associated with the model\n",
    "    tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "    print(f\"Model and tokenizer have been saved to {save_directory}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = Path(os.getcwd()).parent / \"models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer have been saved to c:\\Users\\DELLPHOTO\\depression_detection\\models\n"
     ]
    }
   ],
   "source": [
    "#save_model_and_tokenizer(model, tokenizer, ruta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_inference(text, tokenizer, max_length=128):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,  # Agrega los tokens especiales para RoBERTa '[CLS]' y '[SEP]'\n",
    "        max_length=max_length,    # Trunca o rellena el texto hasta la longitud máxima\n",
    "        padding='max_length',     # Rellena hasta `max_length`\n",
    "        truncation=True,          # Trunca a `max_length` si el texto es más largo\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',      # Retorna tensores de PyTorch\n",
    "    )\n",
    "    return encoding['input_ids'], encoding['attention_mask']\n",
    "\n",
    "def make_prediction(text, model, tokenizer, device):\n",
    "    model.eval()  # Pone el modelo en modo evaluación\n",
    "\n",
    "    input_ids, attention_mask = prepare_text_for_inference(text, tokenizer)\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():  # Desactiva el cálculo de gradientes para inferencia\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=-1)\n",
    "\n",
    "    return probabilities.cpu().numpy(), predicted_class.cpu().numpy()\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_path = ruta\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "model.to(device)  # Asegúrate de que el modelo esté en el mismo dispositivo (CPU o GPU)\n",
    "\n",
    "text = \"Yesterday i went to the movies with my friends\"  # Texto de ejemplo para clasificación\n",
    "probabilities, predicted_class = make_prediction(text, model, tokenizer, device)\n",
    "print(f\"Probabilidades: {probabilities}\")\n",
    "print(f\"Clase Predicha: {predicted_class}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidades: [[9.9740702e-01 2.3933384e-03 1.9965124e-04]]\n",
      "Clase Predicha: [0]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
