{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import RobertaForSequenceClassification, RobertaConfig\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path \n",
    "import os \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "from torch.utils.data import Subset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se leen los datos y se seleccionan las variables que presentan una diferencia\n",
    "# entre los individuos que presentan estres y los que no\n",
    "data_path = Path(os.getcwd()).parent / \"data\"\n",
    "silver_path = data_path / \"silver\" / \"dreadditCleanTrain.csv\"\n",
    "df = pd.read_csv(silver_path, usecols= [\"text\",\"clean_text\",\"clean_text_sentence_sep\",\n",
    "                                        \"singular_pronouns\", \"avg_word_len\",'lex_diversity',\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELLPHOTO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the configuration for the model\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "config = RobertaConfig.from_pretrained(model_name, num_labels=2)  # Set to 2 for binary classification\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, config=config, ignore_mismatched_sizes=True)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "config.id2label = {0: \"no estres\", 1: \"estres\"}\n",
    "config.label2id = {\"no estres\": 0, \"estres\": 1}\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de cómo usar el dataset\n",
    "dataset = SentimentDataset(df['text'].tolist(), df['label'].tolist(), tokenizer, max_length=128)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que 'model' y 'loader' están definidos y configurados correctamente\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Listas para almacenar las métricas\n",
    "losses = []\n",
    "f1_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Epoch 1/5, Train Loss: 0.4480, Validation F1: 0.8322\n",
      "Epoch 2/5, Train Loss: 0.3387, Validation F1: 0.8419\n",
      "Epoch 3/5, Train Loss: 0.2443, Validation F1: 0.8303\n",
      "Epoch 4/5, Train Loss: 0.1771, Validation F1: 0.8361\n",
      "Epoch 5/5, Train Loss: 0.0938, Validation F1: 0.8455\n",
      "Training fold 2\n",
      "Epoch 1/5, Train Loss: 0.2930, Validation F1: 0.9095\n",
      "Epoch 2/5, Train Loss: 0.1883, Validation F1: 0.9866\n",
      "Epoch 3/5, Train Loss: 0.1107, Validation F1: 0.9809\n",
      "Epoch 4/5, Train Loss: 0.0670, Validation F1: 0.9811\n",
      "Epoch 5/5, Train Loss: 0.0597, Validation F1: 0.9675\n",
      "Training fold 3\n",
      "Epoch 1/5, Train Loss: 0.0797, Validation F1: 1.0000\n",
      "Epoch 2/5, Train Loss: 0.0432, Validation F1: 1.0000\n",
      "Epoch 3/5, Train Loss: 0.0299, Validation F1: 0.9911\n",
      "Epoch 4/5, Train Loss: 0.0334, Validation F1: 1.0000\n",
      "Epoch 5/5, Train Loss: 0.0233, Validation F1: 0.9946\n",
      "Average F1 across folds: 0.9440\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Inicializar el KFold\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Convertir labels a numpy para el uso con StratifiedKFold\n",
    "labels = np.array(df['label'].tolist())\n",
    "\n",
    "# Ejecutar la validación cruzada\n",
    "f1_scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
    "    train_subsampler = Subset(dataset, train_idx)\n",
    "    val_subsampler = Subset(dataset, val_idx)\n",
    "    \n",
    "    train_loader = DataLoader(train_subsampler, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_subsampler, batch_size=16, shuffle=False)\n",
    "\n",
    "    print(f\"Training fold {fold+1}\")\n",
    "    fold_f1 = utils.train_and_evaluate(model, train_loader, val_loader, device)\n",
    "    f1_scores.append(fold_f1)\n",
    "\n",
    "print(f\"Average F1 across folds: {np.mean(f1_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = Path(os.getcwd()).parent / \"models\" / \"saved_roberta_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer have been saved to c:\\Users\\DELLPHOTO\\Desktop\\Fran\\depression_detection\\models\\saved_roberta_model\n"
     ]
    }
   ],
   "source": [
    "utils.save_model_and_tokenizer(model, tokenizer, ruta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso\n",
    "\n",
    "model_path = ruta\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "model.to(device)  # Asegúrate de que el modelo esté en el mismo dispositivo (CPU o GPU)\n",
    "\n",
    "text = \"Yesterday i went to the movies with my friends\"  # Texto de ejemplo para clasificación\n",
    "probabilities, predicted_class = utils.make_prediction(text, model, tokenizer, device)\n",
    "print(f\"Probabilidades: {probabilities}\")\n",
    "print(f\"Clase Predicha: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
